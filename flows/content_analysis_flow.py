"""
æ™ºèƒ½å†…å®¹åˆ†æå·¥ä½œæµ - Prefecté›†æˆ
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from prefect import flow, task
from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥é¡¹ç›®åº“
from lib.sitemap import fetch_sitemap, SitemapEntry
from lib.content_extractor import extract_page_content
from lib.content_analyzer import ContentAnalyzer
from lib.content_analysis import ContentAnalysis


@task(log_prints=True, retries=2)
async def analyze_single_url(url: str, analyzer: ContentAnalyzer) -> Optional[ContentAnalysis]:
    """
    åˆ†æå•ä¸ªURLçš„å†…å®¹
    """
    try:
        print(f"ğŸ” å¼€å§‹åˆ†æ: {url}")
        
        # è·å–é¡µé¢å†…å®¹
        import requests
        from bs4 import BeautifulSoup
        
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # æå–æ ‡é¢˜å’Œå†…å®¹
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        # ä½¿ç”¨ç°æœ‰çš„å†…å®¹æå–å™¨
        content = extract_page_content(response.text, url)
        
        # æ¸…ç†HTMLæ ‡ç­¾è·å–çº¯æ–‡æœ¬
        content_soup = BeautifulSoup(content, 'html.parser')
        clean_content = content_soup.get_text()
        
        if len(clean_content.strip()) < 100:
            print(f"âš ï¸ å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡åˆ†æ: {url}")
            return None
        
        # æ‰§è¡Œæ™ºèƒ½åˆ†æ
        analysis = await analyzer.analyze_content(
            content=clean_content,
            title=title_text,
            url=url
        )
        
        print(f"âœ… åˆ†æå®Œæˆ: {url} (è¯„åˆ†: {analysis.reading_score:.1f})")
        return analysis
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥ {url}: {e}")
        return None


@task(log_prints=True)
def save_analysis_results(analyses: List[ContentAnalysis], output_path: str) -> str:
    """
    ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
    """
    # è¿‡æ»¤æ‰Noneç»“æœ
    valid_analyses = [a for a in analyses if a is not None]
    
    if not valid_analyses:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ")
        return ""
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    results = {
        "analyzed_at": datetime.now().isoformat(),
        "total_articles": len(valid_analyses),
        "average_score": sum(a.reading_score for a in valid_analyses) / len(valid_analyses),
        "articles": [analysis.to_dict() for analysis in valid_analyses]
    }
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜JSONæ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {len(valid_analyses)}ç¯‡æ–‡ç« ï¼Œå¹³å‡è¯„åˆ†: {results['average_score']:.2f}")
    
    return output_path


@task(log_prints=True)
def generate_analysis_report(results_path: str) -> str:
    """
    ç”Ÿæˆåˆ†ææŠ¥å‘Š
    """
    try:
        with open(results_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        articles = data.get("articles", [])
        if not articles:
            return "æ²¡æœ‰å¯ç”¨çš„åˆ†ææ•°æ®"
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        scores = [a["reading_score"] for a in articles]
        difficulties = [a["difficulty_level"] for a in articles]
        
        report = f"""
# æ™ºèƒ½å†…å®¹åˆ†ææŠ¥å‘Š

## åŸºæœ¬ç»Ÿè®¡
- åˆ†ææ–‡ç« æ•°é‡: {len(articles)}
- å¹³å‡é˜…è¯»ä»·å€¼è¯„åˆ†: {sum(scores)/len(scores):.2f}/10
- æœ€é«˜è¯„åˆ†: {max(scores):.2f}
- æœ€ä½è¯„åˆ†: {min(scores):.2f}

## éš¾åº¦åˆ†å¸ƒ
- åˆçº§: {difficulties.count('åˆçº§')}ç¯‡
- ä¸­çº§: {difficulties.count('ä¸­çº§')}ç¯‡  
- é«˜çº§: {difficulties.count('é«˜çº§')}ç¯‡

## é«˜ä»·å€¼æ–‡ç« æ¨è (è¯„åˆ†>=8.0)
"""
        
        # ç­›é€‰é«˜ä»·å€¼æ–‡ç« 
        high_value = [a for a in articles if a["reading_score"] >= 8.0]
        high_value.sort(key=lambda x: x["reading_score"], reverse=True)
        
        for i, article in enumerate(high_value[:10], 1):
            report += f"""
### {i}. {article['title']} ({article['reading_score']:.1f}åˆ†)
- URL: {article['url']}
- éš¾åº¦: {article['difficulty_level']}
- é¢„ä¼°é˜…è¯»æ—¶é—´: {article['reading_time_minutes']}åˆ†é’Ÿ
- æ ‡ç­¾: {', '.join(article['tags'])}
- æ‘˜è¦: {article['summary'][:100]}...

"""
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = results_path.replace('.json', '_report.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‹ æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path
        
    except Exception as e:
        print(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
        return ""


@flow(name="æ™ºèƒ½å†…å®¹åˆ†æå·¥ä½œæµ", description="ä»sitemapæå–URLå¹¶è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†æ")
async def content_analysis_flow(
    sitemap_url: str,
    output_dir: str = "output/content_analysis",
    max_articles: int = 20,
    max_concurrent: int = 3,
    llm_config_path: Optional[str] = None
):
    """
    æ™ºèƒ½å†…å®¹åˆ†æä¸»å·¥ä½œæµ
    
    Args:
        sitemap_url: sitemap URL
        output_dir: è¾“å‡ºç›®å½•
        max_articles: æœ€å¤§åˆ†ææ–‡ç« æ•°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        llm_config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„
    """
    print(f"ğŸš€ å¼€å§‹æ™ºèƒ½å†…å®¹åˆ†æå·¥ä½œæµ")
    print(f"ğŸ“ Sitemap URL: {sitemap_url}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    # 1. è·å–sitemapæ¡ç›®
    try:
        sitemap_entries = fetch_sitemap(sitemap_url)
        print(f"ğŸ“„ è·å–åˆ° {len(sitemap_entries)} ä¸ªsitemapæ¡ç›®")
    except Exception as e:
        print(f"âŒ è·å–sitemapå¤±è´¥: {e}")
        return
    
    # 2. é™åˆ¶åˆ†ææ•°é‡
    entries_to_analyze = sitemap_entries[:max_articles]
    print(f"ğŸ¯ å°†åˆ†æå‰ {len(entries_to_analyze)} ç¯‡æ–‡ç« ")
    
    # 3. åˆå§‹åŒ–å†…å®¹åˆ†æå™¨
    analyzer = ContentAnalyzer()
    
    # 4. æ‰¹é‡åˆ†æå†…å®¹
    analysis_tasks = []
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def analyze_with_semaphore(entry: SitemapEntry):
        async with semaphore:
            return await analyze_single_url(entry.url, analyzer)
    
    # åˆ›å»ºæ‰€æœ‰åˆ†æä»»åŠ¡
    for entry in entries_to_analyze:
        task = analyze_with_semaphore(entry)
        analysis_tasks.append(task)
    
    # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
    print(f"ğŸ”„ å¼€å§‹æ‰¹é‡åˆ†æ (å¹¶å‘æ•°: {max_concurrent})")
    analyses = await asyncio.gather(*analysis_tasks, return_exceptions=True)
    
    # å¤„ç†ç»“æœï¼Œè¿‡æ»¤å¼‚å¸¸
    valid_analyses = []
    for result in analyses:
        if isinstance(result, Exception):
            print(f"âš ï¸ åˆ†æä»»åŠ¡å¼‚å¸¸: {result}")
        elif result is not None:
            valid_analyses.append(result)
    
    print(f"âœ… æˆåŠŸåˆ†æ {len(valid_analyses)} ç¯‡æ–‡ç« ")
    
    # 5. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_path = f"{output_dir}/analysis_results_{timestamp}.json"
    
    saved_path = save_analysis_results(valid_analyses, results_path)
    
    # 6. ç”ŸæˆæŠ¥å‘Š
    if saved_path:
        report_path = generate_analysis_report(saved_path)
        
        return {
            "results_path": saved_path,
            "report_path": report_path,
            "articles_analyzed": len(valid_analyses),
        }
    
    return {"error": "åˆ†æå¤±è´¥ï¼Œæ²¡æœ‰ä¿å­˜ç»“æœ"}


@flow(name="æ‰¹é‡ç«™ç‚¹å†…å®¹åˆ†æ", description="åˆ†æå¤šä¸ªç«™ç‚¹çš„å†…å®¹")
async def batch_site_analysis_flow(
    sites_config: List[Dict[str, str]],
    output_dir: str = "output/batch_analysis",
    max_articles_per_site: int = 10
):
    """
    æ‰¹é‡åˆ†æå¤šä¸ªç«™ç‚¹çš„å†…å®¹
    
    Args:
        sites_config: ç«™ç‚¹é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« name, sitemap_url
        output_dir: è¾“å‡ºç›®å½•
        max_articles_per_site: æ¯ä¸ªç«™ç‚¹æœ€å¤§åˆ†ææ–‡ç« æ•°
    """
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡ç«™ç‚¹å†…å®¹åˆ†æ")
    print(f"ğŸ“ å¾…åˆ†æç«™ç‚¹æ•°: {len(sites_config)}")
    
    results = {}
    
    for site in sites_config:
        site_name = site["name"]
        sitemap_url = site["sitemap_url"]
        
        print(f"\nğŸ” åˆ†æç«™ç‚¹: {site_name}")
        
        site_output_dir = f"{output_dir}/{site_name}"
        
        try:
            result = await content_analysis_flow(
                sitemap_url=sitemap_url,
                output_dir=site_output_dir,
                max_articles=max_articles_per_site,
                max_concurrent=2  # é™ä½å¹¶å‘ä»¥é¿å…è¢«é™åˆ¶
            )
            results[site_name] = result
            
        except Exception as e:
            print(f"âŒ ç«™ç‚¹ {site_name} åˆ†æå¤±è´¥: {e}")
            results[site_name] = {"error": str(e)}
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary_path = f"{output_dir}/batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æ‰¹é‡åˆ†æå®Œæˆï¼Œæ±‡æ€»æŠ¥å‘Š: {summary_path}")
    return results


if __name__ == "__main__":
    # ç¤ºä¾‹è¿è¡Œ
    import asyncio
    
    # å•ç«™ç‚¹åˆ†æç¤ºä¾‹
    asyncio.run(content_analysis_flow(
        sitemap_url="https://www.prefect.io/sitemap.xml",
        max_articles=1
    ))