"""
æ™ºèƒ½å†…å®¹åˆ†æå·¥ä½œæµ - Prefecté›†æˆ
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from prefect import flow, task
from dotenv import load_dotenv
load_dotenv()

# å¯¼å…¥é¡¹ç›®åº“
from lib.sitemap import fetch_sitemap, SitemapEntry
from lib.content_extractor import extract_page_content
from lib.content_analyzer import ContentAnalyzer
from lib.content_analysis import ContentAnalysis


@task(log_prints=True, retries=2)
async def analyze_single_url(url: str, analyzer: ContentAnalyzer) -> Optional[ContentAnalysis]:
    """
    åˆ†æå•ä¸ªURLçš„å†…å®¹
    """
    try:
        print(f"ğŸ” å¼€å§‹åˆ†æ: {url}")
        
        # è·å–é¡µé¢å†…å®¹
        import requests
        from bs4 import BeautifulSoup
        
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # æå–æ ‡é¢˜å’Œå†…å®¹
        soup = BeautifulSoup(response.content, 'html.parser')
        title = soup.find('title')
        title_text = title.get_text().strip() if title else "æ— æ ‡é¢˜"
        
        # ä½¿ç”¨ç°æœ‰çš„å†…å®¹æå–å™¨
        content = extract_page_content(response.text, url)
        
        # æ¸…ç†HTMLæ ‡ç­¾è·å–çº¯æ–‡æœ¬
        content_soup = BeautifulSoup(content, 'html.parser')
        clean_content = content_soup.get_text()
        
        if len(clean_content.strip()) < 100:
            print(f"âš ï¸ å†…å®¹å¤ªçŸ­ï¼Œè·³è¿‡åˆ†æ: {url}")
            return None
        
        # æ‰§è¡Œæ™ºèƒ½åˆ†æ
        analysis = await analyzer.analyze_content(
            content=clean_content,
            title=title_text,
            url=url
        )
        
        print(f"âœ… åˆ†æå®Œæˆ: {url} (è¯„åˆ†: {analysis.reading_score:.1f})")
        return analysis
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥ {url}: {e}")
        return None


@task(log_prints=True)
def save_analysis_results(analyses: List[ContentAnalysis], output_path: str) -> str:
    """
    ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶
    """
    # è¿‡æ»¤æ‰Noneç»“æœ
    valid_analyses = [a for a in analyses if a is not None]
    
    if not valid_analyses:
        print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ")
        return ""
    
    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    results = {
        "analyzed_at": datetime.now().isoformat(),
        "articles": [analysis.to_dict() for analysis in valid_analyses]
    }
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜JSONæ–‡ä»¶
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return output_path




@flow(name="æ™ºèƒ½å†…å®¹åˆ†æå·¥ä½œæµ", description="å¯¹æŒ‡å®šURLåˆ—è¡¨è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†æ")
async def content_analysis_flow(
    urls: List[str],
    output_dir: str = "output/content_analysis",
    max_concurrent: int = 3
):
    """
    æ™ºèƒ½å†…å®¹åˆ†æä¸»å·¥ä½œæµ
    
    Args:
        urls: è¦åˆ†æçš„URLåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
    """
    print(f"ğŸš€ å¼€å§‹æ™ºèƒ½å†…å®¹åˆ†æå·¥ä½œæµ")
    print(f"ğŸ“ åˆ†æURLæ•°é‡: {len(urls)}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    
    if not urls:
        print("âŒ æ²¡æœ‰æä¾›è¦åˆ†æçš„URL")
        return {"error": "æ²¡æœ‰æä¾›è¦åˆ†æçš„URL"}
    
    # 1. åˆå§‹åŒ–å†…å®¹åˆ†æå™¨
    analyzer = ContentAnalyzer()
    
    # 2. æ‰¹é‡åˆ†æå†…å®¹
    analysis_tasks = []
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def analyze_with_semaphore(url: str):
        async with semaphore:
            return await analyze_single_url(url, analyzer)
    
    # åˆ›å»ºæ‰€æœ‰åˆ†æä»»åŠ¡
    for url in urls:
        task = analyze_with_semaphore(url)
        analysis_tasks.append(task)
    
    # ç­‰å¾…æ‰€æœ‰åˆ†æå®Œæˆ
    print(f"ğŸ”„ å¼€å§‹æ‰¹é‡åˆ†æ (å¹¶å‘æ•°: {max_concurrent})")
    analyses = await asyncio.gather(*analysis_tasks, return_exceptions=True)
    
    # å¤„ç†ç»“æœï¼Œè¿‡æ»¤å¼‚å¸¸
    valid_analyses = []
    for result in analyses:
        if isinstance(result, Exception):
            print(f"âš ï¸ åˆ†æä»»åŠ¡å¼‚å¸¸: {result}")
        elif result is not None:
            valid_analyses.append(result)
    
    print(f"âœ… æˆåŠŸåˆ†æ {len(valid_analyses)} ç¯‡æ–‡ç« ")
    
    # 3. ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_path = f"{output_dir}/analysis_results_{timestamp}.json"
    
    saved_path = save_analysis_results(valid_analyses, results_path)
    
    # 4. æ‰“å°æ‘˜è¦
    if saved_path:
        return {
            "results_path": saved_path,
            "articles_analyzed": len(valid_analyses),
        }
    
    return {"error": "åˆ†æå¤±è´¥ï¼Œæ²¡æœ‰ä¿å­˜ç»“æœ"}


@flow(name="æ‰¹é‡URLç»„å†…å®¹åˆ†æ", description="åˆ†æå¤šç»„URLçš„å†…å®¹")
async def batch_urls_analysis_flow(
    url_groups: List[Dict[str, any]],
    output_dir: str = "output/batch_analysis"
):
    """
    æ‰¹é‡åˆ†æå¤šç»„URLçš„å†…å®¹
    
    Args:
        url_groups: URLç»„é…ç½®åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« name, urls å­—æ®µ
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡URLç»„å†…å®¹åˆ†æ")
    print(f"ğŸ“ å¾…åˆ†æURLç»„æ•°: {len(url_groups)}")
    
    results = {}
    
    for group in url_groups:
        group_name = group["name"]
        urls = group["urls"]
        max_concurrent = group.get("max_concurrent", 2)
        
        print(f"\nğŸ” åˆ†æURLç»„: {group_name} ({len(urls)} ä¸ªURL)")
        
        group_output_dir = f"{output_dir}/{group_name}"
        
        try:
            result = await content_analysis_flow(
                urls=urls,
                output_dir=group_output_dir,
                max_concurrent=max_concurrent
            )
            results[group_name] = result
            
        except Exception as e:
            print(f"âŒ URLç»„ {group_name} åˆ†æå¤±è´¥: {e}")
            results[group_name] = {"error": str(e)}
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary_path = f"{output_dir}/batch_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… æ‰¹é‡åˆ†æå®Œæˆï¼Œæ±‡æ€»æŠ¥å‘Š: {summary_path}")
    return results


# ä¾¿åˆ©å‡½æ•°
def create_url_list_from_text(text: str) -> List[str]:
    """
    ä»æ–‡æœ¬ä¸­æå–URLåˆ—è¡¨
    æ”¯æŒæ¢è¡Œåˆ†éš”æˆ–é€—å·åˆ†éš”çš„URL
    """
    urls = []
    for line in text.strip().split('\n'):
        line = line.strip()
        if line:
            # å¤„ç†é€—å·åˆ†éš”çš„URL
            if ',' in line:
                urls.extend([url.strip() for url in line.split(',') if url.strip()])
            else:
                urls.append(line)
    
    # è¿‡æ»¤æœ‰æ•ˆçš„URL
    valid_urls = []
    for url in urls:
        if url.startswith(('http://', 'https://')):
            valid_urls.append(url)
    
    return valid_urls


def create_url_list_from_file(file_path: str) -> List[str]:
    """
    ä»æ–‡ä»¶ä¸­è¯»å–URLåˆ—è¡¨
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return create_url_list_from_text(content)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return []


if __name__ == "__main__":
    # ç¤ºä¾‹è¿è¡Œ
    import asyncio
    
    # ç›´æ¥URLåˆ—è¡¨åˆ†æç¤ºä¾‹
    example_urls = [
        "https://www.prefect.io/blog/introducing-prefect-3",
        "https://www.prefect.io/blog/prefect-flows-vs-airflow-dags"
    ]
    
    asyncio.run(content_analysis_flow(urls=example_urls))